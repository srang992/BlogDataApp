Title,Link,Author,Publication Date,Description
Exploratory Data Analysis on Dark Chocolates,https://medium.com/@srang992/exploratory-data-analysis-on-dark-chocolates-bfc2c2d426cd?source=rss-2aa615ea23c3------2,Subhradeep Rang,"Tue, 26 Jul 2022 11:32:22 GMT","From FreepikIntroductionWho doesn’t love chocolate? Everybody does. But not everyone likes dark chocolates as they taste bitter. But if you want to be healthy and want to overcome some stressful situation, this bad guy will give you some relief. Just take a bite of it.Though this analysis is about dark chocolates, I don’t think that this analysis will make you sad. You will come to know many things while reading this article. Just take a cup of coffee and read on.The DataThe data is collected from here. This data consists of more than 2600 rows, where every row contains details about different dark chocolate bars. The column descriptions are given below:id: id number of the reviewmanufacturer : Name of the bar manufacturercompany_location: Location of the manufactureryear_reviewed: From 2006 to 2021bean_origin: Country of origin of the cocoa beansbar_name: Name of the chocolate barcocoa_percent: Cocoa content of the bar (%)num_ingredients: Number of ingredientsingredients: B (Beans), S (Sugar), S* (Sweetener other than sugar or beet sugar), C (Cocoa Butter), (V) Vanilla, (L) Lecithin, (Sa) Saltreview : Summary of most memorable characteristics of the chocolate barrating : 1.0–1.9 Unpleasant, 2.0–2.9 Disappointing, 3.0–3.49 Recommended, 3.5–3.9 Highly Recommended, 4.0–5.0 OutstandingRetrieving Data From Heroku PostgresHere I did some experiments. Rather than importing the data through an excel file using Pandas, I use Heroku Postgres. If you are not interested in learning about it or facing any issues, just go to my Github repo, download the excel file and load it using read_excel() method of Pandas. Otherwise, just read on.Previously Heroku offers a free plan for hosting the PostgreSQL database. This was helpful when you are a beginner or want a quickly hosted database for experiments. But From the December 2022, they removed the free plan. Now they are starting with $5 per month. Now, you can ask me then how we can create a database here? For Students, you have to apply for Github Students Developer pack. From there, you will get $13 per month for 1 year, so use that wisely. If you are not a student, then you have to pay for that.Accessing the Data in pythonTo create a database and add a table to that database, you must read an article by clicking this link. If you previously went through that article, then no problem, just stay on this article. Otherwise, you have to create a database and add the table described in the article and after that, go to the next paragraph.For security purposes, I made a python dictionary with the database credentials, and after that, I accessed those credentials. Here I am using the psycopg2 python library, which helps us to connect with a PostgreSQL database through python.For connecting to the database, we have to just read the pickle file in which the dictionary is saved, and then we have to pass the required arguments in the connect() method of psycopg2.https://medium.com/media/0ca7a173baad13c2752956b297a9200f/hrefHere I keep the names of the keys of the dictionary as same as the name of the arguments in the connect() method. So that, we don’t have to memorize too much. Now, let’s see if everything works perfectly by running a SQL query using pandas.https://medium.com/media/b7b8c5ed8aa54af48f0c2b3d6110ce22/hrefIf everything works as expected, you can see an output like below.Now let’s see some basic info about the data frame. This can be done by the info() method of the pandas dataframe.From the above image, we can see that there are 2530 rows and 10 columns available in the data. Here Cocoa Percent column is of object datatype because of the ‘%’ sign. Also, the column names are too big for some columns. So, let’s remove the ‘%’ sign and short the column names slightly.https://medium.com/media/06f7dc45c8b69407b904263930142416/hrefAfter running the above code, you will see the below output.Now let’s convert the Cocoa Percent column to float data type.https://medium.com/media/47034c08480968bbdf7fb02b1cfe1112/hrefAfter running the code, The output of the code is something like the below:Analyzing the DataKnowing the Basic InformationNow we are ready to do some analysis. Let’s see the summary statistics of the numerical columns.choco_data.describe()From the above summary statistics, we can see that:The average cocoa percentage in the whole dataset is 72%. The minimum and maximum cocoa percentage values are 42% and 100%, respectively. 100% cocoa in a bar of chocolate is bitter for someone who likes sweets.The average chocolate rating for the whole dataset is 3.19. The minimum and maximum ratings that exist in this dataset are 1 and 4, respectively.The summary statistics for reference_no are not so useful here. But from the review_date column, we can say that the dataset contains the reviews from 2006 to 2021.Let’s see the summary statistics of these two after converting them into a categorical column.https://medium.com/media/ae6fdfcee4094ec5b049adba9ac3f1ac/hrefThe output looks like below:From the above result, we can say that:There are 630 unique dark chocolates available in this dataset. From them, the chocolate bar with the reference number 414 appears mostly in the dataset.There are 580 unique manufacturers exist in this dataset. From them, the company named Soma appears more frequently.Most of the companies are located in the U.S.A.Most of the reviews were recorded in 2015.Most of the chocolates are made from those cocoa beans which are originated in Venezuela.The Madagascar chocolate bar is more frequent.The most frequently used ingredients are Beans, Sugar, and Cocoa Beans.The most frequent taste is spicy cocoa. That makes sense as this data is all about dark chocolates.Now, for the sake of simplicity, we are making some functions to keep away ourselves from writing messy codes. The implementations of those functions are given below.https://medium.com/media/9e0ce7f9efc93460d0e6ccc3c9dfca3a/hrefThe usage of those functions is given belowcount_df: This function helps us to join the count of the elements of the categorical column with the main data.number_indicator: this function help us to create a number indicator in Plotly.sort_sliced_dict: This function helps us to sort the python dictionary.Which Chocolate has a Good Average Rating?Here we find out which chocolate bar got the best average rating among all of them.For this, First, I filter those chocolate bars which have records of more than 10. Then I grouped the data based on the bar_name column, and from there, we calculated the mean of the rating column. After that, we sort the values in descending order and display the first 10 rows.https://medium.com/media/fbcd31670fc03637e77d944052de49f6/hrefFrom the above visualization, we can easily tell that the chocolate named Kokoa Kamili has the highest average rating among all of them. Let’s see where this chocolate is mostly manufactured.https://medium.com/media/5515cc936d7415d49b385978cad4c5c0/hrefHmm, it seems that this chocolate is mostly manufactured in the U.S.A. What about the taste of this chocolate? Let’s see.If we see the taste column, we see that a comma separates the taste of specific chocolates.So, we have to hot encode those values. This can be done by the str.get_dummies() method of the panda’s data frame.https://medium.com/media/725a91acb9be21e724828aeee2f28327/hrefWhile one hot encoding the test column, I saw some spelling errors. For example, nut and nuts are the same things, rich cocoa and rich cooa are the same things, but there is a spelling error. So I add those columns with the perfect ones.Now we are ready for the visualization. At first, we have to make a dictionary containing every taste as key and their counts as values. After that, we plot that dictionary. The code is given below.https://medium.com/media/31fe0468049a3058024faa5c4a29e030/hrefIt seems that maximum people are optimistic about the chocolate being fruity. So, that’s all you know about this chocolate. Now, let’s see which company got the most average rating among all of them.Which manufacturer’s chocolates got a good rating?In business, trust is a huge thing. We always want to buy products from a specific company whom we can trust. Let’s see who is lucky here.Here I follow the same procedure I followed for finding the best chocolate with the best average rating.https://medium.com/media/7185f9c5e7a75f2e3aaac7b228baf4b3/hrefIt seems Soma chocolate maker got the best average rating among them. If you google it, you also see that their google rating is decent.Let’s see how much cocoa they used in their chocolates.https://medium.com/media/09784a98771b3f72749e4dfc8d9b4d91/hrefFound some similarities? Of course, you did. I followed the same procedure before. Just here, I also sort the dictionary using the sort_sliced_dict function. The plot is shown below.It seems that Soma Chocomaker uses 70% cocoa in most of the chocolates. Now let’s see which cocoa beans are used in their chocolates.https://medium.com/media/9e043c85c5e10a16bde39a73592d9185/hrefYou can see from the above visualization that there is a bean origin named Blend. This is not a country, of course; this denotes that the Soma chocomaker mostly makes chocolates by blending different types of chocolates.Now, what about the chocolate taste? Let’s see.https://medium.com/media/5e66d73617e0b89b797e2aadd8ae6b08/hrefIt seems that most of the chocolates manufactured by Soma Chocomaker have a creamy taste.Comparison of Average Rating between the chocolate containing Lecithin and that doesn’tWhile researching the ingredient used in chocolates, I discovered that some people don’t like chocolates containing Lecithin, which is allergic to some people. Is that thing reflected in this dataset? Let’s see.A sample of values of the ingredients column is given below.You can see from the above image that the values of the ingredients are in ingredient count- ingredients separated by comma format. So, we have to split those ingredients. You have to do some research to properly split those values. Those values can contain an inconsistency. In my case, I got multiple spaces. So, I strip those spaces and then split them.https://medium.com/media/116169a8d54ea6ab93e66c60edd1573e/hrefIndeed it is reflected in the data. People are ignoring those chocolates containing Lecithin. For this, the average rating for chocolates containing Lecithin is lower than the non-Lecithin one.ConclusionSo, That’s all I got. I know that this article is pretty long, but I think it is worth reading. Now you can easily:Retrieve the data from Heroku PostgresPlot different graphs using plotlyModify the data according to your needsPlot the python dictionary using PlotlyBut the analysis doesn’t end here. If you got something, let me know in the comments. If there is something wrong from my side, I am always here to listen to you.If you read this article on python and plotly carefully, you can see that I don’t use the Plotly library normally. I did some experiments on this. If you want to get a guide about Plotly, let me know."
Master the Web Scraping in 5 minutes,https://medium.com/@srang992/master-the-web-scraping-in-5-minutes-61719f96a722?source=rss-2aa615ea23c3------2,Subhradeep Rang,"Mon, 16 May 2022 14:49:08 GMT","Introduction:From the title, you can easily guess that I am writing this article about Web Scraping. But I am not traditionally writing this article- taking a website and teaching you the scraping using BeautifulSoup or Scrapy library. When we go through a web scraping tutorial, we can easily do those things which are taught in that tutorial, but there is one thing in which we face difficulties when we are going to scrap the web by ourselves - i.e. writing CSS selectors and the XPath. Sometimes, we are trying so many times with different sequences of CSS selector but still, we can’t extract the desired text from it. Then we gave up and quit web scraping. Believe me, I know that pain😣.In this article, we are going to learn how to write XPath and CSS selectors for extracting the desired text properly. If there is anybody who doesn’t know these two things. Don’t worry. At the end of this article, you have everything to start web scraping. Just keep patience and read on.Let’s start coding😎:This article is mainly specific to the Scrapy python library, the most popular web scraping framework. The code is written here all works perfectly with Scrapy.For demonstration purposes, I am using a sample html code which is given below.https://medium.com/media/a62305537cb0202764dd05eb84d50e7e/hrefHey, ‘Choose me!’🤗:We can easily notice from the above code that there is a tree like structure. Remember when we want to import data for data analysis using Pandas, we have to tell Pandas the absolute path of that data. Just like “C://Users//data//tutorial.csv”. In the same procedure we are select the specific portion of webpage using xpath and css. For example, if we want to take the “Choose me!” text from this sample code, we have to just define the xpath in that way -https://medium.com/media/e9abc768fe2fd79180f40cb578ceadcb/hrefHere I use Scrapy’s Selector class which helps us to select the specific element from the HTML. It takes the HTML code as a string as an argument. Let’s see the tree structure for this specific selection.Here I am only displaying those elements in which we are interested. Displaying all the elements makes this illustration a little bit messy 😥. The below illustration defines how we select those elements from this tree structure.‘html/body/div/div/p’- this is the path in which we are interested. For selecting the text in the <p> element, we have to add ./text() for xpath and :: text for css.https://medium.com/media/85eb9314c753e81728a213b1a690294e/hrefHere we specify the XPath and CSS selector. Just one difference, in XPath we are using ‘/’, but for CSS, we are using ‘>’ for defining the required path. In XPath, ‘/’ means we are selecting a single element. In CSS, for selecting a single element, we use the ‘>’ sign. For selecting multiple elements, we use ‘//’ for XPath and Space for CSS.From the above code snippet, you can see that I am using multiple CSS and XPath methods from the sel object. you can write the whole path only in one method, but dividing the path and gluing them with multiple CSS and XPath methods is helpful when you have a large path.https://medium.com/media/b27db3e40c945cbdaed4fbc1a51bbabb/hrefSelecting the 3rd div and its text:Now let’s see the third div element of the body in our example HTML code.We can easily see that there are two texts in the third div element and there is also a text inside the <a> tag. we can select the 3rd div by specifying 3 within the square bracket beside the div in XPath.xpath = '/html/body/div[3]'For css, we have to use ‘nth-of-type’.css = 'html > body > div:nth-of-type(3)'Now we can extract text easily by adding text attribute.https://medium.com/media/5b0eb3e56270e91eeba444982ae94cc2/hrefNow there is one thing. If you use the above code snippet, you will get only the two texts inside the div. you don’t get the text which is inside the <a> tag. Sometimes it happens that we need that text too. Just add // before text() in xpath and for css, you have to add space before text.print(sel.xpath(xpath).xpath('.//text()').extract())print(sel.css(css).css(' ::text()').extract())Sometimes we also want to extract href from <a> tag. For this, we have to specify paths for XPath and CSS defined below.https://medium.com/media/0b111eab0a6ab1f5629fa1a03cb4ae83/hrefSelecting by classes and ids:Sometimes selection with tags is not enough, as we can’t count the position of a specific tag position on a large website. So there is a way to select the tags with their classes and ids. If you are familiar with Web Development, you will know what I mean. In Web Development, we define class and id names with the tags, which makes our life easier while selecting those tags in CSS and Javascript. Exactly for the same purpose, we are also using those class and id names in Scrapy too.https://medium.com/media/707e4b7eb6ca3ea2dcdcfdb3474155af/hrefIn the above code snippet, I described everything about the selection of classes and ids. Just go through the all comments written in the code snippet. I described everything in the code rather than describing it separately for better understanding.Conclusion:And those are everything you need to make your spider in Scrapy like a Pro. You learned about -Writing Xpath and CSS selector properly for scraping a website.Select all texts even if they are in another tag.Select specific HTML tags by their classes and their ids.If you are reading this line, I just want to tell you one thing - Congratulations and Thank you for reading this article so far😇.If you have any queries, just let me know in the comments 😁."
Making a Movie Recommendation App using Streamlit,https://medium.com/@srang992/making-a-movie-recommendation-app-using-streamlit-and-docker-part-1-8dee8983cea9?source=rss-2aa615ea23c3------2,Subhradeep Rang,"Sun, 17 Apr 2022 04:37:35 GMT","Source: ActiveStateIntroduction:For a long time, I had an interest in knowing about the Recommender System. Nowadays, we can hardly find any platform where recommender systems are not in use. Amazon, YouTube, Spotify, Flipkart — every one of them is using this recommender system. So in this article, I am going to make a simple movie recommendation app using Streamlit. But before I start, I want to give a little introduction to the recommendation system.Recommender System - Introduction:In simple words, the recommendation system recommends new items according to our past preferences. The recommendation system can be divided into two parts:Content-Based FilteringCollaborative FilteringIn Content-Based Filtering, the recommender system filters the item according to the item’s content and suggests it to a user. For example, if a user likes a book, then the recommender system recommends a book that is never read by the user but this book is similar to the user’s previously liked book.In Collaborative Filtering, if two or more users have similar preferences, then those items are suggested that are preferred by other users but that item is not used by the current user. If this definition is not so clear to you, don’t worry. I am explaining this to you using the image below.In this image, there are two users, User 1 and User 2. We can easily notice that they both like sweet fruits. User 1 likes Apple and Mango and User 2 likes Mango and Strawberry. So, in that case, the Recommender system suggests Strawberry to User 1 and Apple to User 2 as they don’t taste it yet. I hope this is clear now.Now, Collaborative filtering can be divided into two parts:Item-based collaborative filtering:- In this type of collaborative filtering, items are suggested according to their similarity calculated based on the user’s interaction with other items.User-based collaborative filtering:- In this type of collaborative filtering, items are suggested according to the similarity between users which I illustrate before.Now there is a question:- it seems that Content-based filtering and item-based collaborative filtering are nearly the same. So, what is the difference? In Content-based filtering, we don’t need any User information while recommending an item. But on the other side, Item-based collaborative filtering needs the user’s interaction with the other items, i.e. ratings. It is all about data. If we see that our collected data contains the item’s feature rather than the user interaction, we simply go for content-based filtering. But if we see that the data contains the information about the user ratings, userId, and movieId, just like the Movielens Dataset, then we go for the collaborative filtering.Let’s dive into the main part:Now that’s all about the introduction of a recommendation system. Now let’s move on to the main part:- Making it!For this, I am using the Netflix Movies and TV Shows dataset from Kaggle. You can download the dataset from here.Let’s import the necessary libraries.https://medium.com/media/06b2f690701bd21764a0f0b4cb050b88/hrefIf we print the first 5 rows of the Netflix dataset, the data looks something like this.https://medium.com/media/1b4b783809a1e73e95b66daff9709e23/hrefThis data has 12 columns which are listed below:-show_id:- Unique ID for every Movie / Tv Show.type:- Identifier - A Movie or TV Show.title:- Title of the Movie / TV Show.director:- Director of the Movie.cast:- Actors involved in the movie/show.country:- Country where the movie/show was produced.date_added:- Date it was added on Netflix.release_year:- Actual Release year of the movie/show.rating:- TV Rating of the movie/show.duration:- Total Duration - in minutes or number of seasons.listed_in:- Genredescription:- The summary description.From the above column description, we can easily notice that there is no user information available in this dataset. Here is also a column called rating but this column is not containing the numerical value. For this, we cannot find any similarity using this column. So, we have to make this recommendation system using Text-Based Similarity, i.e. by using their description column. So, here we are only interested in the title and the description column.Now, as we have to work with the text data, so we have to clean those texts before further process. At first, we have to convert all the letters into lowercase. Then all the punctuations should be removed. After that, we tokenize the words, remove the stopwords and lemmatize them.https://medium.com/media/8b626bb64b95d1c12da3273fcffec158/hrefNow after we are done with the cleaning part, let’s convert the words into vectors. I am using TfidfVectorizer from Scikit-learn for converting the words into vectors. The full form of TF-IDF is Term Frequency - Inverse Document Frequency. This process of word vectorization not only convert the words into vectors but also takes care of the word’s importance. The main mathematical formula for TF-IDF is given below.I don’t go deeper about the TF-IDF as this is not the purpose of this article. So, I just give a little definition of this. if you want to learn more about this, there are so many articles. Just go through them.https://medium.com/media/a008093f4fbc2039ac0cee610e686936/hrefJust see from the above code snippet that we ignoring those words which are only in the one document and 70% of the documents. We are ignoring those words contained in the one document as those are not so much useful for finding out the frequency and other words contained in 70% of documents are ignored as those are shadowed by the other words.After vectorization, we are going to make a data frame with that vector array where the title of the movies are in the index and the words are in the columns. then we save that data frame for using this in the Streamlit app.Now there is only one thing left - calculating the similarity. I am using the cosine similarity for calculating the similarities between movies.https://medium.com/media/3312c126104677b3c0df99351fa7c86b/hrefThis function recommend_table takes those movie lists which are enjoyed by an individual and also takes the TF-IDF data frame which we saved previously. Then the cosine similarity is calculated between the user movie list and the other movies in the TF-IDF data frame. After that, this function returns a pandas data frame consisting of movie names and similarity scores sorted in descending order.Now it is the time for making the Streamlit app for this.At first, we import the necessary libraries for making the Streamlit app.https://medium.com/media/625ceb31fe00a3f575536d39c1574d9c/hrefWe are using the recommend_table function which we made previously. One thing I missed. We have to save our movie titles for making a dropdown list in the Streamlit app. Now we have to load the two files from our local storage:-TF-IDF data frame which we saved previously.movie title list saved as a pickle file.https://medium.com/media/96b33fc8e24639e71c31c2a6b561406c/hrefYou can see from the above code snippet that I used a decorator above the load data function - st.cache. this decorator cache the TF-IDF data frame after loading as this TF-IDF data frame has a big memory size. As far as I remember, it has a size of 337 MB! Of course, this is not a joke. This can slow down the app. So we have to cache it.Now we make the main structure of the app.https://medium.com/media/acc11240729c426e333dcf6d372ae75e/hrefHere I am using st.text('') for making a space between widgets. If you want to know about session_state in Streamlit, this is what I got from the Streamlit documentation:-Session State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks.While changing a value in the widget, every time Streamlit loads the whole app to update it, which is not desirable. So, using the session_state in streamlit, we can prevent this. It only loads the widget to update its value. For this, the app performance is also improved.If I run that code, the UI works something like this.Seems awesome, right? Now you have the power of making your recommendation engine. This is an example of a Content-based Recommendation System. You can use any dataset and try it for yourself.In the future, we will deploy this app using Docker as this article is already so big. Until then, stay tuned.If you like this article, please appreciate it by clapping and if I miss something or write something wrong, don’t hesitate to tell me in the comments."
A Beginner’s Guide to Context Manager,https://www.analyticsvidhya.com/blog/2023/01/a-beginners-guide-to-context-manager/,Subhradeep Rang,"Tue, 03 Jan 2023 11:54:58 +0000","This article was published as a part of the Data Science Blogathon. Introduction  When we watch youtube videos or any tutorial about data analysis, we see that the context manager is mainly used in reading or writing some texts from a file so that we don’t have to write the code for closing the file whenever […]
The post A Beginner’s Guide to Context Manager appeared first on Analytics Vidhya."
An Introduction to Julia for Data Analysis,https://www.analyticsvidhya.com/blog/2022/11/an-introduction-to-julia-for-data-analysis/,Subhradeep Rang,"Sun, 27 Nov 2022 19:13:30 +0000","This article was published as a part of the Data Science Blogathon. Introduction Which language do we use when it comes to data analysis? Of course, Python, isn’t it? But there is one more language for data analysis which is growing rapidly. Some of you might guess the language – I am talking about Julia. […]
The post An Introduction to Julia for Data Analysis appeared first on Analytics Vidhya."
Analysis of Restaurants in the United States,https://www.analyticsvidhya.com/blog/2022/10/analysis-of-chain-and-independent-restaurants-in-the-united-states/,Subhradeep Rang,"Thu, 27 Oct 2022 11:07:19 +0000","This article was published as a part of the Data Science Blogathon. Introduction After working for a long time in the office, suddenly, we felt a storm brewing in our stomach, saying Hey! I need food. Then you just come out on the road and start searching for a nearby restaurant – it can be […]
The post Analysis of Restaurants in the United States appeared first on Analytics Vidhya."
Analysis of Australian Shark Attacks,https://www.analyticsvidhya.com/blog/2022/09/analysis-of-australian-shark-attacks/,Subhradeep Rang,"Fri, 30 Sep 2022 05:08:41 +0000","This article was published as a part of the Data Science Blogathon. Introduction Recently I searched for an interesting dataset to learn something new. After searching for a long time, I got a dataset on Shark Attacks in Australia. This dataset contains about 1,100 + shark bites and attempted shark bites between 1791 and early 2022, […]
The post Analysis of Australian Shark Attacks appeared first on Analytics Vidhya."
Making a Multipaged Application using Streamlit,https://www.analyticsvidhya.com/blog/2022/08/making-a-multipaged-application-using-streamlit-on-hold/,Subhradeep Rang,"Thu, 04 Aug 2022 08:30:48 +0000","This article was published as a part of the Data Science Blogathon. Introduction If we want to make a small application in Streamlit, it’s enough to make a single web page-based application. But when the application is large, we must consider splitting the contents into different web pages. This can be done using the st.selectbox […]
The post Making a Multipaged Application using Streamlit appeared first on Analytics Vidhya."
Analysis on Dark Chocolates using Python and Plotly,https://www.analyticsvidhya.com/blog/2022/07/analysis-on-dark-chocolates-using-python-and-plotly/,Subhradeep Rang,"Tue, 26 Jul 2022 11:32:14 +0000","This article was published as a part of the Data Science Blogathon. Introduction Who doesn’t love chocolate? Everybody does. But not everyone likes dark chocolates as they taste bitter. But if you want to be healthy and want to overcome some stressful situation, this bad guy will give you some relief. Just take a bite […]
The post Analysis on Dark Chocolates using Python and Plotly appeared first on Analytics Vidhya."
A Comprehensive Guide to Heroku Postgres,https://www.analyticsvidhya.com/blog/2022/06/a-comprehensive-guide-to-heroku-postgres/,Subhradeep Rang,"Mon, 27 Jun 2022 12:50:58 +0000","This article was published as a part of the Data Science Blogathon. Introduction Recently I was doing an analysis, and in this analysis, I wanted to take the data from a remote database. I previously used Azure, but I had to delete the database after learning. Otherwise, they will deduct money from your Azure balance. […]
The post A Comprehensive Guide to Heroku Postgres appeared first on Analytics Vidhya."
Video Game Clustering Using Python,https://www.analyticsvidhya.com/blog/2022/06/video-game-clustering-using-python/,Subhradeep Rang,"Mon, 06 Jun 2022 17:28:42 +0000","This article was published as a part of the Data Science Blogathon. Introduction on Video Game Clustering In this article, I am going to cluster some of the popular video games according to their given features. I have done this project while learning about the K-Means Clustering. After reading this article, you can able to: Data […]
The post Video Game Clustering Using Python appeared first on Analytics Vidhya."
Customer Churn Prediction of a Telecom Company Using Python,https://www.analyticsvidhya.com/blog/2022/06/customer-churn-prediction-of-a-telecom-company-using-python/,Subhradeep Rang,"Thu, 02 Jun 2022 06:20:53 +0000","This article was published as a part of the Data Science Blogathon. Introduction to Customer Churn Prediction After taking some courses on Data Science, I feel a necessity for applying those skills to some projects. For this, I analyzed and made a machine learning model on a dataset that comes from an Iranian telecom company, […]
The post Customer Churn Prediction of a Telecom Company Using Python appeared first on Analytics Vidhya."
Scraping Jobs on LinkedIn Using Scrapy,https://www.analyticsvidhya.com/blog/2022/05/scraping-jobs-on-linkedin-using-scrapy/,Subhradeep Rang,"Tue, 24 May 2022 14:16:25 +0000","This article was published as a part of the Data Science Blogathon. Introduction Recently I have been working on a personal project in which I want to extract the skills from the resume and match those skills with the job description to figure out how much a candidate is a good fit for a specific […]
The post Scraping Jobs on LinkedIn Using Scrapy appeared first on Analytics Vidhya."
